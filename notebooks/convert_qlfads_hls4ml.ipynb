{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d09936-0031-4931-b27b-4152ec00f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bit = 8\n",
    "\n",
    "qmodels_for_hls4ml_dir = \"qmodels_for_hls4ml/\" + f\"qmodel_totalbit_{total_bit}.h5\"\n",
    "hls_model_output_dir  = './hls_model/lfad_qmodel_' + str(total_bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46ad5a-3359-4d9f-b78c-68ef837aa767",
   "metadata": {},
   "source": [
    "# Load data and define NPLL metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d32437-3992-46b5-8c60-36aac43af254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e791d2-c127-461e-9583-dd204c6613b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_neural_data shape:  (17, 73, 70)\n",
      "inputs2decoder_test shape:  (17, 73, 64)\n"
     ]
    }
   ],
   "source": [
    "test_neural_data = np.load(\"test_neural_data.npy\")\n",
    "inputs2decoder_test = np.load(\"inputs2decoder_test.npy\")\n",
    "\n",
    "print(\"test_neural_data shape: \", test_neural_data.shape)\n",
    "print(\"inputs2decoder_test shape: \", inputs2decoder_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83862d9e-68dc-4b19-b994-e79793af88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def evaluate_NPLL(targets, pred_logrates):\n",
    "    targets = tf.cast(targets, dtype=tf.float32)\n",
    "    logrates = tf.cast(tf.math.log(0.01) + pred_logrates, tf.float32)  # Timestep\n",
    "    npll = tf.nn.log_poisson_loss(targets=targets,log_input=logrates, compute_full_loss=True)\n",
    "    results = tf.reduce_sum(npll, axis=[1, 2]) # sum up each batch seperately\n",
    "    results = tf.reduce_mean(results) # batch mean\n",
    "\n",
    "    return results.numpy() # negative possion loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacc4c5-ca98-46c4-8591-9c7910172723",
   "metadata": {},
   "source": [
    "# Load qkeras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff1fac7-9fb0-4f4d-8a93-7fbb27581ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import load_qmodel\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af46c024-352a-4611-8ada-3c17c920d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_qmodel(\n",
    "    qmodels_for_hls4ml_dir,\n",
    "    compile = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef31e67-8dff-45d8-9a0d-1fabb0fd8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lfad_for_hls4ml\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 73, 70)]     0           []                               \n",
      "                                                                                                  \n",
      " EncoderRNN (QBidirectional)    (None, 128)          52224       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " q_act_postencoder (QActivation  (None, 128)         0           ['EncoderRNN[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " DenseMean (QDense)             (None, 64)           8256        ['q_act_postencoder[0][0]']      \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 73, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " q_act_dense_mean (QActivation)  (None, 64)          0           ['DenseMean[0][0]']              \n",
      "                                                                                                  \n",
      " DecoderGRU (QGRU)              (None, 73, 64)       24960       ['decoder_input[0][0]',          \n",
      "                                                                  'q_act_dense_mean[0][0]']       \n",
      "                                                                                                  \n",
      " q_act_postdecoder (QActivation  (None, 73, 64)      0           ['DecoderGRU[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Dense (QDense)                 (None, 73, 4)        256         ['q_act_postdecoder[0][0]']      \n",
      "                                                                                                  \n",
      " q_act_postdense (QActivation)  (None, 73, 4)        0           ['Dense[0][0]']                  \n",
      "                                                                                                  \n",
      " NeuralDense (QDense)           (None, 73, 70)       350         ['q_act_postdense[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 86,046\n",
      "Trainable params: 86,046\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126d52cf-807a-455a-9efe-7d89b244665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN           quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,2,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,2,0,alpha=1) act=quantized_tanh(8)\n",
      "q_act_postencoder    quantized_bits(8,2,alpha=1)\n",
      "DenseMean            u=64 quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) \n",
      "q_act_dense_mean     quantized_bits(16,6,alpha=1)\n",
      "DecoderGRU           quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) quantized_bits(8,2,0,alpha=1) recurrent act=quantized_sigmoid(8)act=quantized_tanh(8)\n",
      "q_act_postdecoder    quantized_bits(16,6,alpha=1)\n",
      "Dense                u=4 quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) \n",
      "q_act_postdense      quantized_bits(8,2,alpha=1)\n",
      "NeuralDense          u=70 quantized_bits(8,1,0,alpha=1) quantized_bits(8,1,0,alpha=1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_qmodel_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9918e2b0-f8f1-47ed-ba92-60e3e6ec70cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tensorflow.keras import Model\\nimport tensorflow.keras.layers as tfl\\n\\ninput_shape = (73,70)\\ninputs = tfl.Input(shape=input_shape, name = \\'encoder_input\\')\\nencoder_GRU = model.get_layer(\"EncoderRNN\")\\nq_act_postencoder = model.get_layer(\"q_act_postencoder\")\\n\\nmid_result = encoder_GRU(inputs)\\nmid_result = q_act_postencoder(mid_result)\\nencoder_model = Model(inputs = inputs, outputs = mid_result)\\n\\nencoder_model.summary()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "input_shape = (73,70)\n",
    "inputs = tfl.Input(shape=input_shape, name = 'encoder_input')\n",
    "encoder_GRU = model.get_layer(\"EncoderRNN\")\n",
    "q_act_postencoder = model.get_layer(\"q_act_postencoder\")\n",
    "\n",
    "mid_result = encoder_GRU(inputs)\n",
    "mid_result = q_act_postencoder(mid_result)\n",
    "encoder_model = Model(inputs = inputs, outputs = mid_result)\n",
    "\n",
    "encoder_model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f8f169-d64e-403a-864e-894a147517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = encoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee1498-90ea-4ed3-8fbf-8d71552642c5",
   "metadata": {},
   "source": [
    "# Convert to hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec1dee6-c004-427c-ab6c-7284cf0c0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcslab/anaconda3/envs/lfads_hls4ml/lib/python3.8/site-packages/hls4ml/converters/__init__.py:24: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import hls4ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e1681-80b0-45f2-9a43-38ff38ef63d0",
   "metadata": {},
   "source": [
    "## use 32 bit for default precision\n",
    "## this will be used for accumulation bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01825c68-f772-4b80-8d2e-97852f03a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_default_precision = 'ap_fixed<32,16>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e861b33b-2806-45cd-9e37-915f2f80626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "{'name': 'encoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 70]}\n",
      "Layer name: encoder_input, layer type: InputLayer, input shapes: [[None, 73, 70]], output shape: [None, 73, 70]\n",
      "{'name': 'EncoderRNN', 'class_name': 'QBidirectional', 'inputs': ['encoder_input'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': False, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 70, 'n_out': 128, 'initial_state': 0, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 2}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 1}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9be0>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9ee0>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a91c0>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a90a0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9df0>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c72a9310>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c72a9760>, 'subclass_name': 'QGRU', 'merge_mode': 'concat', 'sub_n_out': 64, 'sub_n_state': 64}\n",
      "Layer name: EncoderRNN, layer type: QBidirectional, input shapes: [[None, 73, 70]], output shape: [None, 128]\n",
      "{'name': 'q_act_postencoder', 'class_name': 'Activation', 'inputs': ['EncoderRNN'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postencoder, layer type: Activation, input shapes: [[None, 128]], output shape: [None, 128]\n",
      "{'name': 'DenseMean', 'class_name': 'QDense', 'inputs': ['q_act_postencoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 128, 'n_out': 64, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9f70>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9550>}\n",
      "Layer name: DenseMean, layer type: QDense, input shapes: [[None, 128]], output shape: [None, 64]\n",
      "{'name': 'decoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 64]}\n",
      "Layer name: decoder_input, layer type: InputLayer, input shapes: [[None, 73, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'q_act_dense_mean', 'class_name': 'Activation', 'inputs': ['DenseMean_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_dense_mean, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "{'name': 'DecoderGRU', 'class_name': 'QGRU', 'inputs': ['decoder_input', 'q_act_dense_mean'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': True, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 64, 'n_out': 64, 'initial_state': 1, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 32}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 31}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b80879d0>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9820>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9880>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9f10>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9c40>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c76dbb80>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c76db9d0>}\n",
      "Layer name: DecoderGRU, layer type: QGRU, input shapes: [[None, 73, 64], [None, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'q_act_postdecoder', 'class_name': 'Activation', 'inputs': ['DecoderGRU'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postdecoder, layer type: Activation, input shapes: [[None, 73, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'Dense', 'class_name': 'QDense', 'inputs': ['q_act_postdecoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': False, 'n_in': 64, 'n_out': 4, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9250>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9e50>}\n",
      "Layer name: Dense, layer type: QDense, input shapes: [[None, 73, 64]], output shape: [None, 73, 4]\n",
      "{'name': 'q_act_postdense', 'class_name': 'Activation', 'inputs': ['Dense_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postdense, layer type: Activation, input shapes: [[None, 73, 4]], output shape: [None, 73, 4]\n",
      "{'name': 'NeuralDense', 'class_name': 'QDense', 'inputs': ['q_act_postdense'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 4, 'n_out': 70, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9af0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c76db820>}\n",
      "Layer name: NeuralDense, layer type: QDense, input shapes: [[None, 73, 4]], output shape: [None, 73, 70]\n",
      "[{'name': 'encoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 70]}, {'name': 'EncoderRNN', 'class_name': 'QBidirectional', 'inputs': ['encoder_input'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': False, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 70, 'n_out': 128, 'initial_state': 0, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 2}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 1}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9be0>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9ee0>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a91c0>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a90a0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9df0>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c72a9310>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c72a9760>, 'subclass_name': 'QGRU', 'merge_mode': 'concat', 'sub_n_out': 64, 'sub_n_state': 64}, {'name': 'q_act_postencoder', 'class_name': 'Activation', 'inputs': ['EncoderRNN'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}, {'name': 'DenseMean', 'class_name': 'QDense', 'inputs': ['q_act_postencoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 128, 'n_out': 64, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9f70>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9550>}, {'name': 'DenseMean_linear', 'activation': 'linear', 'class_name': 'Activation'}, {'name': 'decoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 64]}, {'name': 'q_act_dense_mean', 'class_name': 'Activation', 'inputs': ['DenseMean_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}, {'name': 'DecoderGRU', 'class_name': 'QGRU', 'inputs': ['decoder_input', 'q_act_dense_mean'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': True, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 64, 'n_out': 64, 'initial_state': 1, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 32}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 31}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b80879d0>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9820>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9880>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9f10>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9c40>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c76dbb80>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4c76db9d0>}, {'name': 'q_act_postdecoder', 'class_name': 'Activation', 'inputs': ['DecoderGRU'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}, {'name': 'Dense', 'class_name': 'QDense', 'inputs': ['q_act_postdecoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': False, 'n_in': 64, 'n_out': 4, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9250>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9e50>}, {'name': 'Dense_linear', 'activation': 'linear', 'class_name': 'Activation'}, {'name': 'q_act_postdense', 'class_name': 'Activation', 'inputs': ['Dense_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}, {'name': 'NeuralDense', 'class_name': 'QDense', 'inputs': ['q_act_postdense'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 4, 'n_out': 70, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72a9af0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c76db820>}, {'name': 'NeuralDense_linear', 'activation': 'linear', 'class_name': 'Activation'}]\n"
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name', default_precision=hls_default_precision)\n",
    "\n",
    "config['Model']['Strategy'] = 'Resource'\n",
    "config['Model']['ReuseFactor'] = 10000000 #will use N_IN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26eeae4e-a3d2-411c-9a0e-1d1365582b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LayerName': {'DecoderGRU': {'ApplyResetGate': 'after',\n",
      "                              'Direction': 'forward',\n",
      "                              'Precision': {'act': 'ap_fixed<32,16>',\n",
      "                                            'activation': 'fixed<8,1,RND_CONV,SAT>',\n",
      "                                            'bias': 'fixed<8,2>',\n",
      "                                            'recr_act': 'ap_fixed<32,16>',\n",
      "                                            'recurrent_activation': 'ufixed<8,0,RND_CONV,SAT>',\n",
      "                                            'recurrent_bias': 'fixed<8,2>',\n",
      "                                            'recurrent_weight': 'fixed<8,2>',\n",
      "                                            'result': 'ap_fixed<32,16>',\n",
      "                                            'shift': 'ap_fixed<32,16>',\n",
      "                                            'slope': 'ap_fixed<32,16>',\n",
      "                                            'state': 'fixed<8,3>',\n",
      "                                            'weight': 'fixed<8,2>'},\n",
      "                              'Trace': False},\n",
      "               'Dense': {'Precision': {'bias': 'fixed<8,2>',\n",
      "                                       'result': 'ap_fixed<32,16>',\n",
      "                                       'weight': 'fixed<8,2>'},\n",
      "                         'Trace': False},\n",
      "               'DenseMean': {'Precision': {'bias': 'fixed<8,2>',\n",
      "                                           'result': 'ap_fixed<32,16>',\n",
      "                                           'weight': 'fixed<8,2>'},\n",
      "                             'Trace': False},\n",
      "               'DenseMean_linear': {'Precision': {'result': 'ap_fixed<32,16>'},\n",
      "                                    'Trace': False},\n",
      "               'Dense_linear': {'Precision': {'result': 'ap_fixed<32,16>'},\n",
      "                                'Trace': False},\n",
      "               'EncoderRNN': {'ApplyResetGate': 'after',\n",
      "                              'Direction': 'forward',\n",
      "                              'Precision': {'act': 'ap_fixed<32,16>',\n",
      "                                            'backward_bias': 'ap_fixed<32,16>',\n",
      "                                            'backward_recurrent_bias': 'ap_fixed<32,16>',\n",
      "                                            'backward_recurrent_weight': 'ap_fixed<32,16>',\n",
      "                                            'backward_weight': 'ap_fixed<32,16>',\n",
      "                                            'bias': 'fixed<8,2>',\n",
      "                                            'forward_bias': 'ap_fixed<32,16>',\n",
      "                                            'forward_recurrent_bias': 'ap_fixed<32,16>',\n",
      "                                            'forward_recurrent_weight': 'ap_fixed<32,16>',\n",
      "                                            'forward_weight': 'ap_fixed<32,16>',\n",
      "                                            'recr_act': 'ap_fixed<32,16>',\n",
      "                                            'recurrent_activation': 'ufixed<8,0,RND_CONV,SAT>',\n",
      "                                            'recurrent_bias': 'fixed<8,2>',\n",
      "                                            'recurrent_weight': 'fixed<8,2>',\n",
      "                                            'result': 'fixed<8,1,RND_CONV,SAT>',\n",
      "                                            'shift': 'ap_fixed<32,16>',\n",
      "                                            'slope': 'ap_fixed<32,16>',\n",
      "                                            'state': 'fixed<8,3>',\n",
      "                                            'weight': 'fixed<8,2>'},\n",
      "                              'Trace': False},\n",
      "               'NeuralDense': {'Precision': {'bias': 'fixed<8,2>',\n",
      "                                             'result': 'ap_fixed<32,16>',\n",
      "                                             'weight': 'fixed<8,2>'},\n",
      "                               'Trace': False},\n",
      "               'NeuralDense_linear': {'Precision': {'result': 'ap_fixed<32,16>'},\n",
      "                                      'Trace': False},\n",
      "               'decoder_input': {'Precision': {'result': 'ap_fixed<32,16>'},\n",
      "                                 'Trace': False},\n",
      "               'encoder_input': {'Precision': {'result': 'ap_fixed<32,16>'},\n",
      "                                 'Trace': False},\n",
      "               'q_act_dense_mean': {'Precision': {'result': 'fixed<16,7,RND_CONV,SAT>'},\n",
      "                                    'Trace': False},\n",
      "               'q_act_postdecoder': {'Precision': {'result': 'fixed<16,7,RND_CONV,SAT>'},\n",
      "                                     'Trace': False},\n",
      "               'q_act_postdense': {'Precision': {'result': 'fixed<8,3,RND_CONV,SAT>'},\n",
      "                                   'Trace': False},\n",
      "               'q_act_postencoder': {'Precision': {'result': 'fixed<8,3,RND_CONV,SAT>'},\n",
      "                                     'Trace': False}},\n",
      " 'Model': {'BramFactor': 1000000000,\n",
      "           'Precision': 'ap_fixed<32,16>',\n",
      "           'ReuseFactor': 10000000,\n",
      "           'Strategy': 'Resource',\n",
      "           'TraceOutput': False}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c16977e-e24e-46f3-8c33-6daebb40ef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "{'name': 'encoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 70]}\n",
      "Layer name: encoder_input, layer type: InputLayer, input shapes: [[None, 73, 70]], output shape: [None, 73, 70]\n",
      "{'name': 'EncoderRNN', 'class_name': 'QBidirectional', 'inputs': ['encoder_input'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': False, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 70, 'n_out': 128, 'initial_state': 0, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 2}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 1}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bd30>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bdc0>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005b970>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bac0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005ba60>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4b005b7f0>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4b005b910>, 'subclass_name': 'QGRU', 'merge_mode': 'concat', 'sub_n_out': 64, 'sub_n_state': 64}\n",
      "Layer name: EncoderRNN, layer type: QBidirectional, input shapes: [[None, 73, 70]], output shape: [None, 128]\n",
      "{'name': 'q_act_postencoder', 'class_name': 'Activation', 'inputs': ['EncoderRNN'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postencoder, layer type: Activation, input shapes: [[None, 128]], output shape: [None, 128]\n",
      "{'name': 'DenseMean', 'class_name': 'QDense', 'inputs': ['q_act_postencoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 128, 'n_out': 64, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72bd880>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72bdd00>}\n",
      "Layer name: DenseMean, layer type: QDense, input shapes: [[None, 128]], output shape: [None, 64]\n",
      "{'name': 'decoder_input', 'class_name': 'InputLayer', 'data_format': 'channels_last', 'input_shape': [73, 64]}\n",
      "Layer name: decoder_input, layer type: InputLayer, input shapes: [[None, 73, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'q_act_dense_mean', 'class_name': 'Activation', 'inputs': ['DenseMean_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_dense_mean, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "{'name': 'DecoderGRU', 'class_name': 'QGRU', 'inputs': ['decoder_input', 'q_act_dense_mean'], 'data_format': 'channels_last', 'activation': 'hard_tanh', 'use_bias': True, 'return_sequences': True, 'return_state': False, 'recurrent_activation': 'hard_sigmoid', 'time_major': False, 'n_timesteps': 73, 'n_in': 64, 'n_out': 64, 'initial_state': 1, 'recurrent_activation_quantizer': {'class_name': 'quantized_sigmoid', 'config': {'bits': 8, 'symmetric': False, 'use_real_sigmoid': False, 'use_stochastic_rounding': False}, 'shared_object_id': 32}, 'activation_quantizer': {'class_name': 'quantized_tanh', 'config': {'bits': 8, 'symmetric': False, 'use_stochastic_rounding': False, 'use_real_tanh': False}, 'shared_object_id': 31}, 'state_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4c72bdb20>, 'recurrent_bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005b250>, 'recurrent_weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bb50>, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bc10>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005bca0>, 'slope': 0.5, 'shift': 0.5, 'slope_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4b005b490>, 'shift_prec': <hls4ml.model.types.FixedPrecisionType object at 0x76a4b005b2b0>}\n",
      "Layer name: DecoderGRU, layer type: QGRU, input shapes: [[None, 73, 64], [None, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'q_act_postdecoder', 'class_name': 'Activation', 'inputs': ['DecoderGRU'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 16, 'integer': 6, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postdecoder, layer type: Activation, input shapes: [[None, 73, 64]], output shape: [None, 73, 64]\n",
      "{'name': 'Dense', 'class_name': 'QDense', 'inputs': ['q_act_postdecoder'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': False, 'n_in': 64, 'n_out': 4, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b02ea4c0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b02ea190>}\n",
      "Layer name: Dense, layer type: QDense, input shapes: [[None, 73, 64]], output shape: [None, 73, 4]\n",
      "{'name': 'q_act_postdense', 'class_name': 'Activation', 'inputs': ['Dense_linear'], 'data_format': 'channels_last', 'activation': 'linear', 'activation_quantizer': {'class_name': 'linear', 'config': {'bits': 8, 'integer': 2, 'symmetric': 0, 'alpha': 1, 'keep_negative': True, 'use_stochastic_rounding': False, 'qnoise_factor': 1.0}}}\n",
      "Layer name: q_act_postdense, layer type: Activation, input shapes: [[None, 73, 4]], output shape: [None, 73, 4]\n",
      "{'name': 'NeuralDense', 'class_name': 'QDense', 'inputs': ['q_act_postdense'], 'data_format': 'channels_last', 'activation': 'linear', 'use_bias': True, 'n_in': 4, 'n_out': 70, 'weight_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005b0a0>, 'bias_quantizer': <hls4ml.model.types.QKerasQuantizer object at 0x76a4b005b0d0>}\n",
      "Layer name: NeuralDense, layer type: QDense, input shapes: [[None, 73, 4]], output shape: [None, 73, 70]\n",
      "Creating HLS model\n",
      "Using ReuseFactor=N_in: 70\n",
      "Using ReuseFactor=N_in: 64\n",
      "Using ReuseFactor=N_in: 64\n",
      "Using ReuseFactor=N_in: 4\n",
      "Using ReuseFactor=N_in: 128\n",
      "Using ReuseFactor=N_in: 64\n",
      "Using ReuseFactor=N_in: 64\n",
      "Writing HLS project\n",
      "<hls4ml.backends.fpga.fpga_types.VivadoArrayStreamVariable object at 0x76a4c7f7f820>\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir=hls_model_output_dir,\n",
    "                                                       io_type = 'io_array_stream',\n",
    "                                                       #io_type = 'io_parallel',\n",
    "                                                       #backend='VivadoAccelerator', board='pynq-z2')\n",
    "                                                       part='xcu55c-fsvh2892-2L-e')\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ba12461-f4bf-4587-8850-37f96c3c6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npll_qkeras:  1827.6609\n",
      "npll_hls:  1827.6609\n"
     ]
    }
   ],
   "source": [
    "#pred_qkeras = model.predict(test_neural_data)\n",
    "pred_qkeras = model.predict([test_neural_data, inputs2decoder_test])\n",
    "npll_qkeras = evaluate_NPLL(targets=test_neural_data, pred_logrates=pred_qkeras)\n",
    "print(\"npll_qkeras: \", npll_qkeras)\n",
    "\n",
    "#pred_hls = hls_model.predict(test_neural_data)\n",
    "pred_hls = hls_model.predict([test_neural_data, inputs2decoder_test])\n",
    "npll_hls = evaluate_NPLL(targets=test_neural_data, pred_logrates=pred_hls.reshape(17,73,70))\n",
    "print(\"npll_hls: \", npll_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ae9265a-ea90-4362-bf21-2ab87bf25bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(pred_qkeras.flatten(), pred_hls.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88334372-49d0-478e-b358-1f5251687a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14501953,  1.8154297 ,  2.434082  ,  3.1577148 ,  2.400879  ,\n",
       "        3.663086  ,  1.8144531 ,  3.5063477 ,  3.6816406 ,  2.1904297 ,\n",
       "        1.4301758 ,  0.84375   ,  1.8071289 ,  1.0380859 ,  2.8105469 ,\n",
       "        2.2285156 ,  2.4394531 ,  2.7944336 ,  3.4667969 ,  2.7861328 ,\n",
       "        0.88964844,  3.118164  ,  0.9243164 ,  3.1586914 ,  3.7841797 ,\n",
       "        2.3457031 ,  0.8330078 ,  4.1674805 ,  3.3237305 ,  4.225586  ,\n",
       "       -0.53222656,  2.262207  ,  1.9667969 ,  4.008789  ,  3.7998047 ,\n",
       "        2.0385742 ,  3.8666992 ,  0.16601562,  1.315918  ,  2.6972656 ,\n",
       "        3.2333984 ,  0.18017578, -1.1191406 ,  0.9604492 ,  3.234375  ,\n",
       "        2.942871  ,  0.65234375,  3.2011719 ,  1.6928711 ,  2.5151367 ,\n",
       "        3.519043  ,  3.519043  ,  4.25      ,  1.425293  ,  1.4199219 ,\n",
       "        2.0375977 ,  1.4589844 ,  1.559082  ,  0.5361328 ,  1.5522461 ,\n",
       "        1.1308594 ,  2.4760742 ,  2.828125  ,  1.4189453 ,  4.330078  ,\n",
       "        3.203125  ,  3.3261719 ,  1.5439453 ,  1.8457031 ,  0.16113281,\n",
       "        0.7714844 ,  1.8999023 ,  2.5024414 ,  2.7172852 ,  2.354004  ,\n",
       "        3.5585938 ,  1.6508789 ,  3.2495117 ,  3.6142578 ,  2.1679688 ,\n",
       "        1.5078125 ,  1.0053711 ,  1.8227539 ,  1.0566406 ,  2.770996  ,\n",
       "        2.260254  ,  2.449707  ,  2.7436523 ,  3.2680664 ,  2.8486328 ,\n",
       "        1.1040039 ,  3.090332  ,  0.94433594,  3.1435547 ,  3.6679688 ,\n",
       "        2.3251953 ,  1.2543945 ,  3.9536133 ,  3.1748047 ,  3.8310547 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_qkeras.flatten()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a25e534-7fd1-4184-a92f-5e22bffae651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14501953,  1.81542969,  2.43408203,  3.15771484,  2.40087891,\n",
       "        3.66308594,  1.81445312,  3.50634766,  3.68164062,  2.19042969,\n",
       "        1.43017578,  0.84375   ,  1.80712891,  1.03808594,  2.81054688,\n",
       "        2.22851562,  2.43945312,  2.79443359,  3.46679688,  2.78613281,\n",
       "        0.88964844,  3.11816406,  0.92431641,  3.15869141,  3.78417969,\n",
       "        2.34570312,  0.83300781,  4.16748047,  3.32373047,  4.22558594,\n",
       "       -0.53222656,  2.26220703,  1.96679688,  4.00878906,  3.79980469,\n",
       "        2.03857422,  3.86669922,  0.16601562,  1.31591797,  2.69726562,\n",
       "        3.23339844,  0.18017578, -1.11914062,  0.96044922,  3.234375  ,\n",
       "        2.94287109,  0.65234375,  3.20117188,  1.69287109,  2.51513672,\n",
       "        3.51904297,  3.51904297,  4.25      ,  1.42529297,  1.41992188,\n",
       "        2.03759766,  1.45898438,  1.55908203,  0.53613281,  1.55224609,\n",
       "        1.13085938,  2.47607422,  2.828125  ,  1.41894531,  4.33007812,\n",
       "        3.203125  ,  3.32617188,  1.54394531,  1.84570312,  0.16113281,\n",
       "        0.77148438,  1.89990234,  2.50244141,  2.71728516,  2.35400391,\n",
       "        3.55859375,  1.65087891,  3.24951172,  3.61425781,  2.16796875,\n",
       "        1.5078125 ,  1.00537109,  1.82275391,  1.05664062,  2.77099609,\n",
       "        2.26025391,  2.44970703,  2.74365234,  3.26806641,  2.84863281,\n",
       "        1.10400391,  3.09033203,  0.94433594,  3.14355469,  3.66796875,\n",
       "        2.32519531,  1.25439453,  3.95361328,  3.17480469,  3.83105469])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hls.flatten()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231dc242-ac4c-45e2-b545-1d9b4dfdf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = pred_qkeras.flatten()\n",
    "arr2 = pred_hls.flatten()\n",
    "diff_indices = np.where(arr1 != arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cea83274-d59e-4a43-b533-907031cf85ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8efc690d-70ed-419a-8ac4-d00057b31894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices and the differing values from both arrays\n",
    "for idx in diff_indices[0]:\n",
    "    print(f\"Index: {idx}, arr1 value: {arr1[idx]}, arr2 value: {arr2[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d2411-ee20-4a96-ab3b-de2ee85ca184",
   "metadata": {},
   "source": [
    "# Output testbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4061eb47-29c0-4129-b14a-a8682d847ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 5110)\n"
     ]
    }
   ],
   "source": [
    "test_neural_data_flatten = test_neural_data.reshape(test_neural_data.shape[0], -1)\n",
    "print(test_neural_data_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "546f7616-1255-4e4b-a07d-12f0470e1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 4672)\n"
     ]
    }
   ],
   "source": [
    "inputs2decoder_test_flatten = inputs2decoder_test.reshape(inputs2decoder_test.shape[0], -1)\n",
    "print(inputs2decoder_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51051e3-7d8c-4b0e-a8a2-899215b1f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 9782)\n"
     ]
    }
   ],
   "source": [
    "# concat in dim1, as hls4ml will process the encoder input first, and then decoder\n",
    "x_test = np.concatenate([test_neural_data_flatten, inputs2decoder_test_flatten], axis=1)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63f5aac5-5da7-4675-a2e9-065fc24601a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 5110)\n"
     ]
    }
   ],
   "source": [
    "pred_qkeras_flatten = pred_qkeras.reshape(pred_qkeras.shape[0], -1)\n",
    "print(pred_qkeras_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c706096-cbbe-4e59-a29a-a513f463c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save testbench files for hls project\n",
    "\n",
    "#write x_test to tb_input_features.dat\n",
    "INPUT_FILE = hls_model_output_dir + '/tb_data/tb_input_features.dat'\n",
    "np.savetxt(INPUT_FILE, x_test, delimiter=' ', fmt='%f')\n",
    "\n",
    "#write x_test to tb_input_features.dat\n",
    "OUTPUT_FILE = hls_model_output_dir + '/tb_data/tb_output_predictions.dat'\n",
    "np.savetxt(OUTPUT_FILE, pred_qkeras_flatten, delimiter=' ', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d3bc3-32ce-4844-aeca-0e6fe025e536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
